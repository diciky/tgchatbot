import logging
import re
from typing import List, Dict, Set
from telegram import Update
from telegram.ext import ContextTypes
from database import get_db

logger = logging.getLogger(__name__)

class KeywordMonitor:
    def __init__(self):
        self.db = get_db()
        self.sensitive_words = set()  # æ•æ„Ÿè¯é›†åˆ
        self.load_sensitive_words()
    
    def load_sensitive_words(self):
        """ä»æ•°æ®åº“åŠ è½½æ•æ„Ÿè¯åˆ—è¡¨"""
        try:
            words = self.db.sensitive_words.find()
            self.sensitive_words = {word['word'] for word in words}
        except Exception as e:
            logger.error(f"åŠ è½½æ•æ„Ÿè¯å¤±è´¥: {e}")
            self.sensitive_words = set()
    
    def add_sensitive_word(self, word: str) -> bool:
        """æ·»åŠ æ•æ„Ÿè¯"""
        try:
            if word not in self.sensitive_words:
                self.db.sensitive_words.insert_one({'word': word})
                self.sensitive_words.add(word)
                return True
            return False
        except Exception as e:
            logger.error(f"æ·»åŠ æ•æ„Ÿè¯å¤±è´¥: {e}")
            return False
    
    def remove_sensitive_word(self, word: str) -> bool:
        """åˆ é™¤æ•æ„Ÿè¯"""
        try:
            if word in self.sensitive_words:
                self.db.sensitive_words.delete_one({'word': word})
                self.sensitive_words.remove(word)
                return True
            return False
        except Exception as e:
            logger.error(f"åˆ é™¤æ•æ„Ÿè¯å¤±è´¥: {e}")
            return False
    
    def check_sensitive_content(self, text: str) -> List[str]:
        """æ£€æŸ¥æ–‡æœ¬ä¸­çš„æ•æ„Ÿè¯"""
        found_words = []
        for word in self.sensitive_words:
            if word in text:
                found_words.append(word)
        return found_words
    
    def analyze_user_behavior(self, user_id: int, group_id: int = None) -> Dict:
        """åˆ†æç”¨æˆ·è¡Œä¸º"""
        try:
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            query = {'user_id': user_id}
            if group_id:
                query['group_id'] = group_id
            
            # è·å–ç”¨æˆ·æœ€è¿‘çš„æ¶ˆæ¯
            recent_messages = list(self.db.messages.find(query).sort('timestamp', -1).limit(100))
            
            if not recent_messages:
                return {}
            
            # åˆ†ææ¶ˆæ¯ç‰¹å¾
            total_messages = len(recent_messages)
            sensitive_count = 0
            keyword_count = 0
            message_types = {}
            
            for msg in recent_messages:
                # æ£€æŸ¥æ•æ„Ÿè¯
                if self.check_sensitive_content(msg.get('text', '')):
                    sensitive_count += 1
                
                # ç»Ÿè®¡æ¶ˆæ¯ç±»å‹
                msg_type = msg.get('type', 'text')
                message_types[msg_type] = message_types.get(msg_type, 0) + 1
            
            # è®¡ç®—æ•æ„Ÿè¯ä½¿ç”¨ç‡
            sensitive_ratio = sensitive_count / total_messages if total_messages > 0 else 0
            
            return {
                'total_messages': total_messages,
                'sensitive_count': sensitive_count,
                'sensitive_ratio': sensitive_ratio,
                'message_types': message_types,
                'risk_level': self._calculate_risk_level(sensitive_ratio)
            }
        except Exception as e:
            logger.error(f"åˆ†æç”¨æˆ·è¡Œä¸ºå¤±è´¥: {e}")
            return {}
    
    def _calculate_risk_level(self, sensitive_ratio: float) -> str:
        """è®¡ç®—é£é™©ç­‰çº§"""
        if sensitive_ratio > 0.5:
            return "é«˜é£é™©"
        elif sensitive_ratio > 0.2:
            return "ä¸­é£é™©"
        elif sensitive_ratio > 0:
            return "ä½é£é™©"
        return "æ­£å¸¸"

# åˆ›å»ºç›‘æ§å®ä¾‹
monitor = KeywordMonitor()

async def add_sensitive_word_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """æ·»åŠ æ•æ„Ÿè¯å‘½ä»¤"""
    if not context.args:
        await update.message.reply_text("è¯·æä¾›è¦æ·»åŠ çš„æ•æ„Ÿè¯ï¼")
        return
    
    word = ' '.join(context.args)
    if monitor.add_sensitive_word(word):
        await update.message.reply_text(f"æˆåŠŸæ·»åŠ æ•æ„Ÿè¯: {word}")
    else:
        await update.message.reply_text("æ·»åŠ æ•æ„Ÿè¯å¤±è´¥æˆ–è¯¥è¯å·²å­˜åœ¨")

async def remove_sensitive_word_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """åˆ é™¤æ•æ„Ÿè¯å‘½ä»¤"""
    if not context.args:
        await update.message.reply_text("è¯·æä¾›è¦åˆ é™¤çš„æ•æ„Ÿè¯ï¼")
        return
    
    word = ' '.join(context.args)
    if monitor.remove_sensitive_word(word):
        await update.message.reply_text(f"æˆåŠŸåˆ é™¤æ•æ„Ÿè¯: {word}")
    else:
        await update.message.reply_text("åˆ é™¤æ•æ„Ÿè¯å¤±è´¥æˆ–è¯¥è¯ä¸å­˜åœ¨")

async def check_behavior_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """æ£€æŸ¥ç”¨æˆ·è¡Œä¸ºå‘½ä»¤"""
    if not update.effective_chat.type == 'group':
        await update.message.reply_text("æ­¤å‘½ä»¤åªèƒ½åœ¨ç¾¤ç»„ä¸­ä½¿ç”¨ï¼")
        return
    
    user_id = update.effective_user.id
    group_id = update.effective_chat.id
    
    behavior = monitor.analyze_user_behavior(user_id, group_id)
    if not behavior:
        await update.message.reply_text("æ— æ³•è·å–ç”¨æˆ·è¡Œä¸ºæ•°æ®")
        return
    
    response = (
        f"ğŸ“Š ç”¨æˆ·è¡Œä¸ºåˆ†ææŠ¥å‘Š\n\n"
        f"æ€»æ¶ˆæ¯æ•°: {behavior['total_messages']}\n"
        f"æ•æ„Ÿè¯ä½¿ç”¨æ¬¡æ•°: {behavior['sensitive_count']}\n"
        f"æ•æ„Ÿè¯ä½¿ç”¨ç‡: {behavior['sensitive_ratio']:.2%}\n"
        f"é£é™©ç­‰çº§: {behavior['risk_level']}\n\n"
        f"æ¶ˆæ¯ç±»å‹åˆ†å¸ƒ:\n"
    )
    
    for msg_type, count in behavior['message_types'].items():
        response += f"- {msg_type}: {count}\n"
    
    await update.message.reply_text(response) 